{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and define helper functions"
      ],
      "metadata": {
        "id": "nEXjwqUtUJE1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mk8GQk-UDYM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jcDaEZHDUOpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "  \"\"\"\n",
        "  return (n_samples, z_dim, 1, 1)\n",
        "  \"\"\"\n",
        "  # view(len(noise), self.z_dim, 1, 1)\n",
        "  noise = torch.randn(n_samples, z_dim, device=device)\n",
        "  return noise.view(len(noise), z_dim, 1, 1)"
      ],
      "metadata": {
        "id": "3yv9AcjXW1bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downlaod dataset and preprocess it"
      ],
      "metadata": {
        "id": "W-BeesLCFQuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Models(Generator and Discriminator)"
      ],
      "metadata": {
        "id": "R-NFyprVURib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])"
      ],
      "metadata": {
        "id": "ZCxKQ2PlFV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(MNIST('.', download=True, transform=transform), batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIH17SmwFTe8",
        "outputId": "f9bee6a6-8ef4-4b0c-8ab2-b14e4b1d1c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5454696.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 160205.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1513099.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5115609.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "7oWB5w4-UYrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_block(input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "  if final_layer:\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "        nn.BatchNorm2d(output_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "wzv4edoOVBjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim=10, image_channel=1, hidden_dim=64):\n",
        "    super(Generator, self).__init__()\n",
        "    self.gen_layers = nn.Sequential(\n",
        "        # (1, 10, 1, 1)\n",
        "        gen_block(z_dim, hidden_dim*4, kernel_size=3, stride=2),\n",
        "        # (1, C, 3, 3)\n",
        "        gen_block(hidden_dim*4, hidden_dim*2, kernel_size=4, stride=1),\n",
        "        # (1, C, 6, 6)\n",
        "        gen_block(hidden_dim*2, hidden_dim, kernel_size=3, stride=2),\n",
        "        # (1, C, 13, 13)\n",
        "        gen_block(hidden_dim, image_channel, kernel_size=4, stride=2, final_layer=True)\n",
        "        # (1, image_channel, 28, 28)\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      noise: (None, z_dim, 1, 1)\n",
        "    Output:\n",
        "      fake: (None, 3, 28, 28)\n",
        "    \"\"\"\n",
        "    fake = self.gen_layers(noise)\n",
        "    return fake"
      ],
      "metadata": {
        "id": "fCGhvDfOUQnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "CWlmozKjb65g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def disc_block(input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "  if final_layer:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "        nn.BatchNorm2d(output_channels),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )"
      ],
      "metadata": {
        "id": "qHgFIQtQXe--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, image_channel=1, hidden_dim=16):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.disc_layers = nn.Sequential(\n",
        "        disc_block(image_channel, hidden_dim),\n",
        "        disc_block(hidden_dim, hidden_dim*2),\n",
        "        disc_block(hidden_dim*2, 1, final_layer=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, image):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      image: (None, 1, 28, 28)\n",
        "    Outputs:\n",
        "      dis_red: (None, 1)\n",
        "    \"\"\"\n",
        "    disc_pred = self.disc_layers(image)\n",
        "    return disc_pred.view(len(disc_pred), -1)\n"
      ],
      "metadata": {
        "id": "tllhJ8L8-yER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZFBVy-cvCrl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "CSoWYd-hDa3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "device='cpu'\n",
        "z_dim = 10\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "lr = 0.0002\n",
        "z_dim = 64\n",
        "display_step = 500"
      ],
      "metadata": {
        "id": "o2Bhm2teEd68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function - BCE"
      ],
      "metadata": {
        "id": "TvLGO_e2WH02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "29hDkFQ3Hf50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define optimizers for Generator and Discriminator"
      ],
      "metadata": {
        "id": "0_YKTQVcWK5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "disc = Discriminator().to(device)\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))"
      ],
      "metadata": {
        "id": "UXrY9uP0GgZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initalize weights for Generator and Discriminator"
      ],
      "metadata": {
        "id": "ewaqTsPfWRbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "    # set mean and variance as 0.0 and 0.02 respectively\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "gen = gen.apply(weights_init)\n",
        "disc = disc.apply(weights_init)"
      ],
      "metadata": {
        "id": "vP0RK2rZMTat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_generator_loss = 0\n",
        "mean_discriminator_loss = 0\n",
        "curr_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for real, _ in tqdm(dataloader):\n",
        "    curr_batch_size = len(real)\n",
        "    real = real.to(device)\n",
        "\n",
        "    # update Discriminator\n",
        "    disc_opt.zero_grad()\n",
        "    noise = get_noise(curr_batch_size, z_dim, device=device)\n",
        "    fake = gen(noise)\n",
        "\n",
        "    disc_fake_pred = disc(fake.detach())\n",
        "    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
        "\n",
        "    disc_real_pred = disc(real)\n",
        "    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
        "\n",
        "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
        "\n",
        "    disc_loss.backward()\n",
        "    disc_opt.step()\n",
        "\n",
        "    # update Generator\n",
        "    gen_opt.zero_grad()\n",
        "\n",
        "    noise = get_noise(curr_batch_size, z_dim, device=device)\n",
        "    fake2 = gen(noise)\n",
        "    disc_fake2_pred = disc(fake2)\n",
        "\n",
        "    gen_loss = criterion(disc_fake2_pred, torch.ones_like(disc_fake2_pred))\n",
        "\n",
        "    gen_loss.backward()\n",
        "    gen_opt.step()\n",
        "\n",
        "    # for display\n",
        "    mean_discriminator_loss += disc_loss.item() / display_step\n",
        "    mean_generator_loss += gen_loss.item() / display_step\n",
        "\n",
        "    if curr_step % display_step == 0 and curr_step > 0:\n",
        "      print(f\"Epoch: {curr_step}: Generator Loss: {mean_generator_loss}, Discriminator Loss: {mean_discriminator_loss}\")\n",
        "      noise = get_noise(curr_batch_size, z_dim, device=device)\n",
        "      fake = gen(noise)\n",
        "      show_tensor_images(fake)\n",
        "      show_tensor_images(real)\n",
        "      mean_generator_loss = 0\n",
        "      mean_discriminator_loss = 0\n",
        "\n",
        "    curr_step += 1"
      ],
      "metadata": {
        "id": "tGVpzFh9DbrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}