{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2pNQmA1wYi9W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Encoder"
      ],
      "metadata": {
        "id": "MGFoCyxVli0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Embedding` and `Positional Encoding`"
      ],
      "metadata": {
        "id": "YwWZuKELvcZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_positional_encoding(max_length, d_model):\n",
        "    assert d_model % 2 == 0, \"Dimension model must be even\"\n",
        "\n",
        "    pos = torch.arange(0, max_length).unsqueeze(1) # (max_length, 1)\n",
        "    pos_expanded = pos.repeat(1, d_model // 2) # (max_length, d_model // 2)\n",
        "\n",
        "    power = torch.arange(0, d_model, 2).float() / d_model\n",
        "    div_term = torch.pow(10000, power).unsqueeze(0) # (1, d_model // 2)\n",
        "    div_term_expanded = div_term.repeat(max_length, 1)  # (max_length, d_model // 2)\n",
        "\n",
        "    pe = torch.zeros(max_length, d_model) # (max_length, d_model)\n",
        "    pe[:, 0::2] = torch.sin(pos_expanded / div_term_expanded) # (max_length, d_model // 2)\n",
        "    pe[:, 1::2] = torch.cos(pos_expanded / div_term_expanded) # (max_length, d_model // 2)\n",
        "\n",
        "    return pe\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, d_model):\n",
        "    super(Embedding, self).__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    self.pos_encoding = create_positional_encoding(max_length, d_model) # (seq_length, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" Apply embedding and positional encoding to the input\n",
        "\n",
        "    Input:\n",
        "      x: (N, seq_length)\n",
        "    Output:\n",
        "      x: (N, seq_length, d_model)\n",
        "    \"\"\"\n",
        "    # apply embedding\n",
        "    x = self.embedding(x)\n",
        "    # add positional encoding\n",
        "    x += self.pos_encoding[:x.size(1)]\n",
        "    return x"
      ],
      "metadata": {
        "id": "SIjFZzpileZt"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SelfMutliHeadAttention` and `Add&Norm`"
      ],
      "metadata": {
        "id": "3_PKutMdvjnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_output, _ = self.mha(query=x, key=x, value=x)\n",
        "    x = self.layer_norm(x + attn_output)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mpWnFnCXuMCQ"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FeedForward` and `Add&Norm`"
      ],
      "metadata": {
        "id": "mypxOq8wvpxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, dropout):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.seq = nn.ModuleList([\n",
        "        nn.Linear(d_model, 2 * d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(2 * d_model, d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    ])\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    original_x = x\n",
        "    for layer in self.seq:\n",
        "      x = layer(x)\n",
        "    out = x + original_x\n",
        "    return self.layernorm(out)\n"
      ],
      "metadata": {
        "id": "_vtaxatuuNc2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Layer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super(Encoder_Layer, self).__init__()\n",
        "    self.self_attention = SelfAttention(d_model, num_heads, dropout)\n",
        "    self.ff = FeedForward(d_model, dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ff(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "85oERHjbuW0w"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, d_model, num_heads, num_layers, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, max_length, d_model)\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        Encoder_Layer(d_model, num_heads, dropout) for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    for enc_layer in self.encoder_layers:\n",
        "      x = enc_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VlHeAlN9uX05"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Decoder"
      ],
      "metadata": {
        "id": "dAv8efGdYxor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CasualMultiHeadAttention` and `Add&Norm`"
      ],
      "metadata": {
        "id": "XIeAxLhEvv71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_size = 128\n",
        "causal_mask = torch.zeros(mask_size, mask_size)\n",
        "for i in range(mask_size):\n",
        "    for j in range(mask_size):\n",
        "        if i < j:\n",
        "            causal_mask[i, j] = float('-inf')\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super(CausalSelfAttention, self).__init__()\n",
        "    self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_output, _ = self.mha(query=x, key=x, value=x, attn_mask=causal_mask, is_causal=True)\n",
        "    x = self.layer_norm(x + attn_output)\n",
        "    return x"
      ],
      "metadata": {
        "id": "eoI6vZLDaeXV"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CrossMultiHeadAttention` and+ `Add&Norm`"
      ],
      "metadata": {
        "id": "eQB40HGihAAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super(CrossAttention, self).__init__()\n",
        "    self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x, image_embedding):\n",
        "    attn_output, _ = self.mha(query=x, key=image_embedding, value=image_embedding)\n",
        "    x = self.layer_norm(x + attn_output)\n",
        "    return x\n",
        ""
      ],
      "metadata": {
        "id": "WfQC6yIGatWN"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FeedForward` and `Add&Norm Layer`"
      ],
      "metadata": {
        "id": "Kd-7LR0WhK79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, dropout):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.seq = nn.ModuleList([\n",
        "        nn.Linear(d_model, 2 * d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(2 * d_model, d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    ])\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    original_x = x\n",
        "    for layer in self.seq:\n",
        "      x = layer(x)\n",
        "    out = x + original_x\n",
        "    return self.layernorm(out)\n",
        ""
      ],
      "metadata": {
        "id": "7z5xqQteawty"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.casual_attention = CausalSelfAttention(d_model, num_heads, dropout)\n",
        "    self.cross_attention = CrossAttention(d_model, num_heads, dropout)\n",
        "    self.ff = FeedForward(d_model, dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    x = self.casual_attention(x)\n",
        "    x = self.cross_attention(x, encoder_output)\n",
        "    x = self.ff(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7HaI99wva6UV"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, d_model, num_heads, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.decoder_embedding = Embedding(vocab_size, max_length, d_model)\n",
        "    self.decoder_layers = nn.ModuleList([\n",
        "        DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)\n",
        "    ])\n",
        "    self.last_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, deocder_input, encoder_output):\n",
        "    '''\n",
        "    deocder_input: (N, max_length)\n",
        "    encoder_output: (N, max_length, d_model)\n",
        "    '''\n",
        "    x = self.decoder_embedding(deocder_input) # (N, max_length, d_model)\n",
        "\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      x = dec_layer(x, encoder_output)\n",
        "\n",
        "    x = self.last_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "i7GZxzdDhSui"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create TransFormer"
      ],
      "metadata": {
        "id": "e4EHLXTHutNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransFormer(nn.Module):\n",
        "  def __init__(self, vocab_size=5000, max_length=128, d_model=512, num_heads=8, num_layers=4, dropout=0.5):\n",
        "    super(TransFormer, self).__init__()\n",
        "    self.encoder = Encoder(vocab_size, max_length, d_model, num_heads, num_layers, dropout)\n",
        "    self.decoder = Decoder(vocab_size, max_length, d_model, num_heads, num_layers, dropout)\n",
        "\n",
        "  def forward(self, encoder_input, decoder_input):\n",
        "    encoder_output = self.encoder(encoder_input)\n",
        "    model_output = self.decoder(decoder_input, encoder_output)\n",
        "    return model_output"
      ],
      "metadata": {
        "id": "C2md8FeNql8_"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = torch.randint(0, 5000, size=(32, 128))\n",
        "encoder_input = torch.randint(0, 5000, size=(32, 128))\n",
        "transformer = TransFormer()\n",
        "output = transformer(encoder_input, decoder_input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFqCAtPCtPml",
        "outputId": "1ad6d9ba-0f6b-4ef8-90ce-b2607b6474af"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128, 5000])\n"
          ]
        }
      ]
    }
  ]
}